# Kubernetes监控与日志管理深度解析

## 摘要

本文深入解析Kubernetes集群的监控与日志管理技术，涵盖指标采集、可视化展示、日志处理、分布式追踪等核心技术。
通过形式化建模、代码实现和最佳实践，为Kubernetes集群的可观测性提供完整的解决方案。

## 目录

- [Kubernetes监控与日志管理深度解析](#kubernetes监控与日志管理深度解析)
  - [摘要](#摘要)
  - [目录](#目录)
  - [1. 监控体系架构](#1-监控体系架构)
    - [1.1 监控架构设计](#11-监控架构设计)
    - [1.2 监控组件架构](#12-监控组件架构)
  - [2. 指标采集与处理](#2-指标采集与处理)
    - [2.1 指标类型与分类](#21-指标类型与分类)
      - [2.1.1 指标分类](#211-指标分类)
      - [2.1.2 指标采集方式](#212-指标采集方式)
    - [2.2 指标处理与聚合](#22-指标处理与聚合)
      - [2.2.1 指标聚合策略](#221-指标聚合策略)
  - [3. 可视化与告警](#3-可视化与告警)
    - [3.1 Grafana仪表盘设计](#31-grafana仪表盘设计)
      - [3.1.1 仪表盘架构](#311-仪表盘架构)
      - [3.1.2 关键指标展示](#312-关键指标展示)
    - [3.2 告警规则设计](#32-告警规则设计)
      - [3.2.1 告警规则分类](#321-告警规则分类)
      - [3.2.2 告警路由配置](#322-告警路由配置)
  - [4. 日志管理技术](#4-日志管理技术)
    - [4.1 日志采集架构](#41-日志采集架构)
      - [4.1.1 日志采集方式](#411-日志采集方式)
      - [4.1.2 日志处理管道](#412-日志处理管道)
    - [4.2 日志存储与检索](#42-日志存储与检索)
      - [4.2.1 Elasticsearch配置](#421-elasticsearch配置)
      - [4.2.2 日志索引策略](#422-日志索引策略)
  - [5. 分布式追踪](#5-分布式追踪)
    - [5.1 追踪架构设计](#51-追踪架构设计)
      - [5.1.1 OpenTelemetry集成](#511-opentelemetry集成)
      - [5.1.2 应用追踪集成](#512-应用追踪集成)
  - [6. 代码实现与工具](#6-代码实现与工具)
    - [6.1 Rust实现：监控指标收集器](#61-rust实现监控指标收集器)
    - [6.2 Golang实现：日志处理管道](#62-golang实现日志处理管道)
  - [7. 最佳实践](#7-最佳实践)
    - [7.1 监控最佳实践](#71-监控最佳实践)
      - [7.1.1 指标设计原则](#711-指标设计原则)
      - [7.1.2 告警设计原则](#712-告警设计原则)
    - [7.2 日志管理最佳实践](#72-日志管理最佳实践)
      - [7.2.1 日志结构化](#721-日志结构化)
      - [7.2.2 日志保留策略](#722-日志保留策略)
    - [7.3 性能优化](#73-性能优化)
      - [7.3.1 监控性能优化](#731-监控性能优化)
      - [7.3.2 日志性能优化](#732-日志性能优化)
  - [8. 总结](#8-总结)
    - [8.1 技术要点总结](#81-技术要点总结)
    - [8.2 实施建议](#82-实施建议)

## 1. 监控体系架构

### 1.1 监控架构设计

**分层监控架构**：

```text
┌─────────────────────────────────────────────────────────────┐
│                    应用层监控                                │
│  ┌─────────────┐  ┌─────────────┐  ┌─────────────┐         │
│  │   业务指标   │  │   用户指标   │  │   性能指标   │         │
│  └─────────────┘  └─────────────┘  └─────────────┘         │
└─────────────────────────────────────────────────────────────┘
                              │
┌─────────────────────────────────────────────────────────────┐
│                   Kubernetes层监控                          │
│  ┌─────────────┐  ┌─────────────┐  ┌─────────────┐         │
│  │   Pod指标    │  │   Node指标   │  │   集群指标   │         │
│  └─────────────┘  └─────────────┘  └─────────────┘         │
└─────────────────────────────────────────────────────────────┘
                              │
┌─────────────────────────────────────────────────────────────┐
│                    基础设施监控                              │
│  ┌─────────────┐  ┌─────────────┐  ┌─────────────┐         │
│  │   CPU/内存   │  │   存储I/O    │  │   网络流量   │         │
│  └─────────────┘  └─────────────┘  └─────────────┘         │
└─────────────────────────────────────────────────────────────┘
```

### 1.2 监控组件架构

**核心组件关系**：

- **Metrics Server**: 集群资源指标聚合
- **Prometheus**: 指标收集和存储
- **Grafana**: 可视化展示
- **Alertmanager**: 告警管理
- **kube-state-metrics**: Kubernetes对象状态指标

## 2. 指标采集与处理

### 2.1 指标类型与分类

#### 2.1.1 指标分类

**四大黄金指标**：

1. **延迟（Latency）**: 请求处理时间
2. **流量（Traffic）**: 系统负载
3. **错误（Errors）**: 错误率
4. **饱和度（Saturation）**: 资源利用率

**Kubernetes特定指标**：

- **Pod指标**: CPU、内存、网络、存储使用率
- **Node指标**: 节点资源状态、健康状态
- **集群指标**: 集群整体状态、调度性能

#### 2.1.2 指标采集方式

**Pull模式**：

```yaml
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: web-service-monitor
  namespace: monitoring
spec:
  selector:
    matchLabels:
      app: web
  endpoints:
  - port: metrics
    interval: 15s
    path: /metrics
```

**Push模式**：

```yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: pushgateway-config
data:
  pushgateway.yml: |
    global:
      external_labels:
        cluster: production
    rule_files:
    - "alert_rules.yml"
```

### 2.2 指标处理与聚合

#### 2.2.1 指标聚合策略

**时间序列聚合**：

- 平均值聚合
- 分位数聚合
- 计数聚合
- 求和聚合

**标签聚合**：

- 按命名空间聚合
- 按应用聚合
- 按节点聚合

## 3. 可视化与告警

### 3.1 Grafana仪表盘设计

#### 3.1.1 仪表盘架构

**分层仪表盘设计**：

- **概览仪表盘**: 集群整体状态
- **应用仪表盘**: 应用性能指标
- **基础设施仪表盘**: 资源使用情况
- **告警仪表盘**: 告警状态和趋势

#### 3.1.2 关键指标展示

**集群健康指标**：

```json
{
  "dashboard": {
    "title": "Kubernetes Cluster Health",
    "panels": [
      {
        "title": "Pod Status",
        "type": "stat",
        "targets": [
          {
            "expr": "sum(kube_pod_status_phase{phase=\"Running\"})",
            "legendFormat": "Running Pods"
          }
        ]
      },
      {
        "title": "Node CPU Usage",
        "type": "graph",
        "targets": [
          {
            "expr": "100 - (avg(rate(node_cpu_seconds_total{mode=\"idle\"}[5m])) * 100)",
            "legendFormat": "CPU Usage %"
          }
        ]
      }
    ]
  }
}
```

### 3.2 告警规则设计

#### 3.2.1 告警规则分类

**关键告警规则**：

```yaml
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: k8s-critical-alerts
  namespace: monitoring
spec:
  groups:
  - name: k8s.pod.availability
    rules:
    - alert: PodCrashLoopBackOff
      expr: kube_pod_status_phase{phase="Failed"} > 0
      for: 5m
      labels:
        severity: critical
      annotations:
        summary: "Pod {{ $labels.pod }} in namespace {{ $labels.namespace }} is in CrashLoopBackOff"
        description: "Pod {{ $labels.pod }} has been restarting for more than 5 minutes"
        
  - name: k8s.node.resources
    rules:
    - alert: NodeMemoryPressure
      expr: (1 - (node_memory_MemAvailable_bytes / node_memory_MemTotal_bytes)) > 0.9
      for: 10m
      labels:
        severity: warning
      annotations:
        summary: "Node {{ $labels.instance }} memory pressure is high"
        description: "Node {{ $labels.instance }} memory usage is above 90%"
```

#### 3.2.2 告警路由配置

**Alertmanager配置**：

```yaml
global:
  smtp_smarthost: 'localhost:587'
  smtp_from: 'alerts@company.com'

route:
  group_by: ['alertname']
  group_wait: 10s
  group_interval: 10s
  repeat_interval: 1h
  receiver: 'web.hook'
  routes:
  - match:
      severity: critical
    receiver: 'critical-alerts'
  - match:
      severity: warning
    receiver: 'warning-alerts'

receivers:
- name: 'web.hook'
  webhook_configs:
  - url: 'http://127.0.0.1:5001/'
- name: 'critical-alerts'
  email_configs:
  - to: 'admin@company.com'
    subject: 'Critical Alert: {{ .GroupLabels.alertname }}'
- name: 'warning-alerts'
  email_configs:
  - to: 'ops@company.com'
    subject: 'Warning Alert: {{ .GroupLabels.alertname }}'
```

## 4. 日志管理技术

### 4.1 日志采集架构

#### 4.1.1 日志采集方式

**DaemonSet方式**：

```yaml
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: fluent-bit
  namespace: logging
spec:
  selector:
    matchLabels:
      app: fluent-bit
  template:
    metadata:
      labels:
        app: fluent-bit
    spec:
      containers:
      - name: fluent-bit
        image: fluent/fluent-bit:2.0
        volumeMounts:
        - name: varlog
          mountPath: /var/log
        - name: varlibdockercontainers
          mountPath: /var/lib/docker/containers
          readOnly: true
        - name: fluent-bit-config
          mountPath: /fluent-bit/etc/
      volumes:
      - name: varlog
        hostPath:
          path: /var/log
      - name: varlibdockercontainers
        hostPath:
          path: /var/lib/docker/containers
      - name: fluent-bit-config
        configMap:
          name: fluent-bit-config
```

#### 4.1.2 日志处理管道

**Fluent Bit配置**：

```ini
[SERVICE]
    Flush         1
    Log_Level     info
    Daemon        off
    Parsers_File  parsers.conf
    HTTP_Server   On
    HTTP_Listen   0.0.0.0
    HTTP_Port     2020

[INPUT]
    Name              tail
    Path              /var/log/containers/*.log
    Parser            docker
    Tag               kube.*
    Refresh_Interval  5
    Mem_Buf_Limit     50MB
    Skip_Long_Lines   On

[FILTER]
    Name                kubernetes
    Match               kube.*
    Kube_URL            https://kubernetes.default.svc:443
    Kube_CA_File        /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
    Kube_Token_File     /var/run/secrets/kubernetes.io/serviceaccount/token
    Kube_Tag_Prefix     kube.var.log.containers.
    Merge_Log           On
    Keep_Log            Off
    K8S-Logging.Parser  On
    K8S-Logging.Exclude Off

[OUTPUT]
    Name  es
    Match *
    Host  elasticsearch.logging.svc.cluster.local
    Port  9200
    Index kubernetes-logs
    Type  _doc
```

### 4.2 日志存储与检索

#### 4.2.1 Elasticsearch配置

**Elasticsearch集群配置**：

```yaml
apiVersion: elasticsearch.k8s.elastic.co/v1
kind: Elasticsearch
metadata:
  name: elasticsearch
  namespace: logging
spec:
  version: 8.5.0
  nodeSets:
  - name: default
    count: 3
    config:
      node.roles: ["master", "data", "ingest"]
      node.store.allow_mmap: false
    podTemplate:
      spec:
        containers:
        - name: elasticsearch
          resources:
            requests:
              memory: 4Gi
              cpu: 1000m
            limits:
              memory: 4Gi
              cpu: 1000m
```

#### 4.2.2 日志索引策略

**索引生命周期管理**：

```json
{
  "policy": {
    "phases": {
      "hot": {
        "actions": {
          "rollover": {
            "max_size": "50GB",
            "max_age": "1d"
          }
        }
      },
      "warm": {
        "min_age": "1d",
        "actions": {
          "allocate": {
            "number_of_replicas": 0
          }
        }
      },
      "cold": {
        "min_age": "7d",
        "actions": {
          "allocate": {
            "number_of_replicas": 0
          }
        }
      },
      "delete": {
        "min_age": "30d"
      }
    }
  }
}
```

## 5. 分布式追踪

### 5.1 追踪架构设计

#### 5.1.1 OpenTelemetry集成

**OpenTelemetry Collector配置**：

```yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: otel-collector-config
  namespace: monitoring
data:
  otel-collector-config.yaml: |
    receivers:
      otlp:
        protocols:
          grpc:
            endpoint: 0.0.0.0:4317
          http:
            endpoint: 0.0.0.0:4318
      jaeger:
        protocols:
          grpc:
            endpoint: 0.0.0.0:14250
          thrift_http:
            endpoint: 0.0.0.0:14268
          thrift_compact:
            endpoint: 0.0.0.0:6831
          thrift_binary:
            endpoint: 0.0.0.0:6832

    processors:
      batch:
        timeout: 1s
        send_batch_size: 1024
      memory_limiter:
        limit_mib: 512

    exporters:
      jaeger:
        endpoint: jaeger-collector:14250
        tls:
          insecure: true
      logging:
        loglevel: debug

    service:
      pipelines:
        traces:
          receivers: [otlp, jaeger]
          processors: [memory_limiter, batch]
          exporters: [jaeger, logging]
```

#### 5.1.2 应用追踪集成

**Java应用追踪配置**：

```java
@Configuration
public class TracingConfig {
    
    @Bean
    public OpenTelemetry openTelemetry() {
        return OpenTelemetrySdk.builder()
            .setTracerProvider(
                SdkTracerProvider.builder()
                    .addSpanProcessor(BatchSpanProcessor.builder(
                        OtlpGrpcSpanExporter.builder()
                            .setEndpoint("http://otel-collector:4317")
                            .build())
                        .build())
                    .build())
            .build();
    }
    
    @Bean
    public Tracer tracer(OpenTelemetry openTelemetry) {
        return openTelemetry.getTracer("my-application");
    }
}
```

## 6. 代码实现与工具

### 6.1 Rust实现：监控指标收集器

```rust
use std::sync::atomic::{AtomicU64, Ordering};
use std::collections::HashMap;
use std::time::{Duration, Instant};
use serde::{Deserialize, Serialize};

/// Kubernetes监控指标
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct K8sMetrics {
    pub pod_count: AtomicU64,
    pub node_count: AtomicU64,
    pub cpu_usage: AtomicU64,
    pub memory_usage: AtomicU64,
    pub network_rx: AtomicU64,
    pub network_tx: AtomicU64,
    pub storage_usage: AtomicU64,
}

impl K8sMetrics {
    pub fn new() -> Self {
        Self {
            pod_count: AtomicU64::new(0),
            node_count: AtomicU64::new(0),
            cpu_usage: AtomicU64::new(0),
            memory_usage: AtomicU64::new(0),
            network_rx: AtomicU64::new(0),
            network_tx: AtomicU64::new(0),
            storage_usage: AtomicU64::new(0),
        }
    }

    /// 计算集群健康分数
    pub fn calculate_health_score(&self) -> f64 {
        let pod_count = self.pod_count.load(Ordering::Relaxed) as f64;
        let node_count = self.node_count.load(Ordering::Relaxed) as f64;
        let cpu_usage = self.cpu_usage.load(Ordering::Relaxed) as f64;
        let memory_usage = self.memory_usage.load(Ordering::Relaxed) as f64;
        
        // 健康分数计算：基于资源利用率和可用性
        let resource_score = 100.0 - ((cpu_usage + memory_usage) / 2.0);
        let availability_score = (pod_count / node_count) * 100.0;
        
        (resource_score + availability_score) / 2.0
    }

    /// 检测异常指标
    pub fn detect_anomalies(&self) -> Vec<String> {
        let mut anomalies = Vec::new();
        
        let cpu_usage = self.cpu_usage.load(Ordering::Relaxed) as f64;
        let memory_usage = self.memory_usage.load(Ordering::Relaxed) as f64;
        
        if cpu_usage > 80.0 {
            anomalies.push("High CPU usage detected".to_string());
        }
        
        if memory_usage > 90.0 {
            anomalies.push("High memory usage detected".to_string());
        }
        
        let pod_count = self.pod_count.load(Ordering::Relaxed);
        let node_count = self.node_count.load(Ordering::Relaxed);
        
        if pod_count > node_count * 10 {
            anomalies.push("High pod density detected".to_string());
        }
        
        anomalies
    }
}

/// 监控数据收集器
pub struct MetricsCollector {
    metrics: K8sMetrics,
    collection_interval: Duration,
}

impl MetricsCollector {
    pub fn new(interval: Duration) -> Self {
        Self {
            metrics: K8sMetrics::new(),
            collection_interval: interval,
        }
    }

    /// 开始收集指标
    pub async fn start_collection(&self) -> Result<(), Box<dyn std::error::Error>> {
        let mut interval = tokio::time::interval(self.collection_interval);
        
        loop {
            interval.tick().await;
            
            // 收集Pod指标
            self.collect_pod_metrics().await?;
            
            // 收集Node指标
            self.collect_node_metrics().await?;
            
            // 收集资源指标
            self.collect_resource_metrics().await?;
            
            // 检查异常
            let anomalies = self.metrics.detect_anomalies();
            if !anomalies.is_empty() {
                self.handle_anomalies(anomalies).await?;
            }
        }
    }

    async fn collect_pod_metrics(&self) -> Result<(), Box<dyn std::error::Error>> {
        // 实现Pod指标收集逻辑
        // 这里应该调用Kubernetes API获取Pod信息
        Ok(())
    }

    async fn collect_node_metrics(&self) -> Result<(), Box<dyn std::error::Error>> {
        // 实现Node指标收集逻辑
        // 这里应该调用Kubernetes API获取Node信息
        Ok(())
    }

    async fn collect_resource_metrics(&self) -> Result<(), Box<dyn std::error::Error>> {
        // 实现资源指标收集逻辑
        // 这里应该收集CPU、内存、网络、存储等指标
        Ok(())
    }

    async fn handle_anomalies(&self, anomalies: Vec<String>) -> Result<(), Box<dyn std::error::Error>> {
        // 处理异常情况，发送告警
        for anomaly in anomalies {
            println!("Alert: {}", anomaly);
            // 这里应该发送告警到Alertmanager
        }
        Ok(())
    }
}
```

### 6.2 Golang实现：日志处理管道

```go
package main

import (
    "context"
    "encoding/json"
    "fmt"
    "log"
    "sync"
    "time"
)

// LogEntry 日志条目
type LogEntry struct {
    Timestamp   time.Time              `json:"timestamp"`
    Level       string                 `json:"level"`
    Message     string                 `json:"message"`
    Namespace   string                 `json:"namespace"`
    PodName     string                 `json:"pod_name"`
    ContainerName string               `json:"container_name"`
    Labels      map[string]string      `json:"labels"`
    Fields      map[string]interface{} `json:"fields"`
}

// LogProcessor 日志处理器
type LogProcessor struct {
    inputChan   chan LogEntry
    outputChan  chan LogEntry
    filters     []LogFilter
    transformers []LogTransformer
    mu          sync.RWMutex
}

// LogFilter 日志过滤器接口
type LogFilter interface {
    Filter(entry LogEntry) bool
}

// LogTransformer 日志转换器接口
type LogTransformer interface {
    Transform(entry LogEntry) LogEntry
}

// NewLogProcessor 创建日志处理器
func NewLogProcessor() *LogProcessor {
    return &LogProcessor{
        inputChan:   make(chan LogEntry, 1000),
        outputChan:  make(chan LogEntry, 1000),
        filters:     make([]LogFilter, 0),
        transformers: make([]LogTransformer, 0),
    }
}

// AddFilter 添加过滤器
func (lp *LogProcessor) AddFilter(filter LogFilter) {
    lp.mu.Lock()
    defer lp.mu.Unlock()
    lp.filters = append(lp.filters, filter)
}

// AddTransformer 添加转换器
func (lp *LogProcessor) AddTransformer(transformer LogTransformer) {
    lp.mu.Lock()
    defer lp.mu.Unlock()
    lp.transformers = append(lp.transformers, transformer)
}

// Process 处理日志
func (lp *LogProcessor) Process(ctx context.Context) error {
    for {
        select {
        case <-ctx.Done():
            return ctx.Err()
        case entry := <-lp.inputChan:
            // 应用过滤器
            if lp.shouldFilter(entry) {
                continue
            }
            
            // 应用转换器
            transformedEntry := lp.applyTransformers(entry)
            
            // 发送到输出通道
            select {
            case lp.outputChan <- transformedEntry:
            case <-ctx.Done():
                return ctx.Err()
            }
        }
    }
}

// shouldFilter 检查是否应该过滤日志
func (lp *LogProcessor) shouldFilter(entry LogEntry) bool {
    lp.mu.RLock()
    defer lp.mu.RUnlock()
    
    for _, filter := range lp.filters {
        if !filter.Filter(entry) {
            return true
        }
    }
    return false
}

// applyTransformers 应用转换器
func (lp *LogProcessor) applyTransformers(entry LogEntry) LogEntry {
    lp.mu.RLock()
    defer lp.mu.RUnlock()
    
    transformedEntry := entry
    for _, transformer := range lp.transformers {
        transformedEntry = transformer.Transform(transformedEntry)
    }
    return transformedEntry
}

// SendLog 发送日志到处理器
func (lp *LogProcessor) SendLog(entry LogEntry) {
    select {
    case lp.inputChan <- entry:
    default:
        log.Printf("Warning: log processor input channel is full, dropping log entry")
    }
}

// GetOutputChannel 获取输出通道
func (lp *LogProcessor) GetOutputChannel() <-chan LogEntry {
    return lp.outputChan
}

// LevelFilter 日志级别过滤器
type LevelFilter struct {
    AllowedLevels []string
}

func (lf *LevelFilter) Filter(entry LogEntry) bool {
    for _, level := range lf.AllowedLevels {
        if entry.Level == level {
            return true
        }
    }
    return false
}

// NamespaceFilter 命名空间过滤器
type NamespaceFilter struct {
    AllowedNamespaces []string
}

func (nf *NamespaceFilter) Filter(entry LogEntry) bool {
    for _, namespace := range nf.AllowedNamespaces {
        if entry.Namespace == namespace {
            return true
        }
    }
    return false
}

// TimestampTransformer 时间戳转换器
type TimestampTransformer struct{}

func (tt *TimestampTransformer) Transform(entry LogEntry) LogEntry {
    entry.Timestamp = time.Now()
    return entry
}

// FieldExtractor 字段提取器
type FieldExtractor struct {
    FieldsToExtract []string
}

func (fe *FieldExtractor) Transform(entry LogEntry) LogEntry {
    if entry.Fields == nil {
        entry.Fields = make(map[string]interface{})
    }
    
    for _, field := range fe.FieldsToExtract {
        if value, exists := entry.Labels[field]; exists {
            entry.Fields[field] = value
        }
    }
    
    return entry
}

// LogStorage 日志存储接口
type LogStorage interface {
    Store(entry LogEntry) error
    Query(query LogQuery) ([]LogEntry, error)
}

// LogQuery 日志查询
type LogQuery struct {
    Namespace   string
    PodName     string
    Level       string
    StartTime   time.Time
    EndTime     time.Time
    Limit       int
}

// ElasticsearchStorage Elasticsearch存储实现
type ElasticsearchStorage struct {
    endpoint string
    index    string
}

func NewElasticsearchStorage(endpoint, index string) *ElasticsearchStorage {
    return &ElasticsearchStorage{
        endpoint: endpoint,
        index:    index,
    }
}

func (es *ElasticsearchStorage) Store(entry LogEntry) error {
    // 实现Elasticsearch存储逻辑
    log.Printf("Storing log entry to Elasticsearch: %+v", entry)
    return nil
}

func (es *ElasticsearchStorage) Query(query LogQuery) ([]LogEntry, error) {
    // 实现Elasticsearch查询逻辑
    log.Printf("Querying logs from Elasticsearch: %+v", query)
    return []LogEntry{}, nil
}

// LogPipeline 日志处理管道
type LogPipeline struct {
    processor *LogProcessor
    storage   LogStorage
}

func NewLogPipeline(storage LogStorage) *LogPipeline {
    return &LogPipeline{
        processor: NewLogProcessor(),
        storage:   storage,
    }
}

func (pipeline *LogPipeline) Start(ctx context.Context) error {
    // 启动日志处理器
    go func() {
        if err := pipeline.processor.Process(ctx); err != nil {
            log.Printf("Error in log processor: %v", err)
        }
    }()
    
    // 启动存储写入器
    go func() {
        for {
            select {
            case <-ctx.Done():
                return
            case entry := <-pipeline.processor.GetOutputChannel():
                if err := pipeline.storage.Store(entry); err != nil {
                    log.Printf("Error storing log entry: %v", err)
                }
            }
        }
    }()
    
    return nil
}

func (pipeline *LogPipeline) SendLog(entry LogEntry) {
    pipeline.processor.SendLog(entry)
}

func main() {
    // 创建Elasticsearch存储
    storage := NewElasticsearchStorage("http://elasticsearch:9200", "kubernetes-logs")
    
    // 创建日志处理管道
    pipeline := NewLogPipeline(storage)
    
    // 添加过滤器
    levelFilter := &LevelFilter{
        AllowedLevels: []string{"ERROR", "WARN", "INFO"},
    }
    pipeline.processor.AddFilter(levelFilter)
    
    namespaceFilter := &NamespaceFilter{
        AllowedNamespaces: []string{"default", "production"},
    }
    pipeline.processor.AddFilter(namespaceFilter)
    
    // 添加转换器
    timestampTransformer := &TimestampTransformer{}
    pipeline.processor.AddTransformer(timestampTransformer)
    
    fieldExtractor := &FieldExtractor{
        FieldsToExtract: []string{"app", "version", "environment"},
    }
    pipeline.processor.AddTransformer(fieldExtractor)
    
    // 启动管道
    ctx := context.Background()
    if err := pipeline.Start(ctx); err != nil {
        log.Fatalf("Failed to start log pipeline: %v", err)
    }
    
    // 模拟日志输入
    for i := 0; i < 10; i++ {
        entry := LogEntry{
            Timestamp:     time.Now(),
            Level:         "INFO",
            Message:       fmt.Sprintf("Test log message %d", i),
            Namespace:     "default",
            PodName:       fmt.Sprintf("test-pod-%d", i),
            ContainerName: "test-container",
            Labels: map[string]string{
                "app":         "test-app",
                "version":     "1.0.0",
                "environment": "development",
            },
        }
        
        pipeline.SendLog(entry)
        time.Sleep(100 * time.Millisecond)
    }
    
    // 等待处理完成
    time.Sleep(2 * time.Second)
}
```

## 7. 最佳实践

### 7.1 监控最佳实践

#### 7.1.1 指标设计原则

**指标设计原则**：

1. **相关性**: 指标应该与业务目标相关
2. **可操作性**: 指标应该能够指导行动
3. **可理解性**: 指标应该易于理解
4. **可比较性**: 指标应该支持历史比较

#### 7.1.2 告警设计原则

**告警设计原则**：

1. **及时性**: 告警应该及时触发
2. **准确性**: 告警应该准确反映问题
3. **可操作性**: 告警应该提供明确的行动指导
4. **可抑制性**: 告警应该支持抑制和静默

### 7.2 日志管理最佳实践

#### 7.2.1 日志结构化

**结构化日志格式**：

```json
{
  "timestamp": "2025-01-27T10:30:00Z",
  "level": "INFO",
  "message": "User login successful",
  "namespace": "user-service",
  "pod": "user-service-7d4b8c9f6-xyz12",
  "container": "user-service",
  "labels": {
    "app": "user-service",
    "version": "1.2.3",
    "environment": "production"
  },
  "fields": {
    "user_id": "12345",
    "ip_address": "192.168.1.100",
    "user_agent": "Mozilla/5.0...",
    "response_time_ms": 150
  }
}
```

#### 7.2.2 日志保留策略

**日志保留策略**：

- **热数据**: 7天，快速访问
- **温数据**: 30天，标准访问
- **冷数据**: 90天，慢速访问
- **归档数据**: 1年，离线存储

### 7.3 性能优化

#### 7.3.1 监控性能优化

**性能优化策略**：

- 使用批量处理减少API调用
- 实现指标缓存减少重复计算
- 使用异步处理提高吞吐量
- 优化查询减少存储压力

#### 7.3.2 日志性能优化

**性能优化策略**：

- 使用异步写入提高性能
- 实现日志缓冲减少I/O
- 使用压缩减少存储空间
- 优化索引提高查询性能

## 8. 总结

### 8.1 技术要点总结

**监控技术要点**：

- 分层监控架构设计
- 四大黄金指标监控
- 智能告警规则设计
- 可视化仪表盘构建

**日志管理要点**：

- 结构化日志设计
- 分布式日志采集
- 高效日志处理管道
- 智能日志检索

### 8.2 实施建议

**实施步骤**：

1. **规划阶段**: 设计监控和日志架构
2. **部署阶段**: 部署监控和日志组件
3. **配置阶段**: 配置指标采集和告警规则
4. **优化阶段**: 优化性能和调整策略
5. **维护阶段**: 持续监控和改进

**关键成功因素**：

- 明确监控目标和指标
- 建立完善的告警机制
- 实现日志标准化
- 持续优化和改进

---

*本文档提供了Kubernetes监控与日志管理的完整解决方案，包含理论指导、代码实现和最佳实践。*
