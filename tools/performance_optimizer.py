#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ€§èƒ½ä¼˜åŒ–å·¥å…·
æä¾›ç³»ç»Ÿæ€§èƒ½åˆ†æã€ä¼˜åŒ–å»ºè®®å’Œè‡ªåŠ¨ä¼˜åŒ–åŠŸèƒ½
"""

import os
import sys
import json
import time
import psutil
import threading
import multiprocessing
from pathlib import Path
from typing import Dict, List, Optional, Any, Tuple, Callable
from datetime import datetime, timedelta
import argparse
import cProfile
import pstats
import io
from dataclasses import dataclass, asdict
from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor
import gc
import tracemalloc

@dataclass
class PerformanceMetrics:
    """æ€§èƒ½æŒ‡æ ‡"""
    timestamp: str
    cpu_usage: float
    memory_usage: float
    memory_peak: float
    disk_io_read: int
    disk_io_write: int
    network_io_sent: int
    network_io_recv: int
    process_count: int
    thread_count: int

@dataclass
class OptimizationResult:
    """ä¼˜åŒ–ç»“æœ"""
    optimization_type: str
    before_metrics: Dict[str, Any]
    after_metrics: Dict[str, Any]
    improvement_percent: float
    description: str
    recommendations: List[str]

class PerformanceOptimizer:
    """æ€§èƒ½ä¼˜åŒ–å·¥å…·"""
    
    def __init__(self, root_dir: str = "."):
        self.root_dir = Path(root_dir)
        self.tools_dir = self.root_dir / "tools"
        self.reports_dir = self.root_dir / "tools" / "reports"
        self.reports_dir.mkdir(parents=True, exist_ok=True)
        
        # æ€§èƒ½æ•°æ®å­˜å‚¨
        self.performance_history = []
        self.optimization_results = []
        
        # ç›‘æ§çŠ¶æ€
        self.monitoring_active = False
        self.monitor_thread = None
        
        # æ€§èƒ½åŸºå‡†
        self.baseline_metrics = None
        
    def collect_performance_metrics(self) -> PerformanceMetrics:
        """æ”¶é›†æ€§èƒ½æŒ‡æ ‡"""
        try:
            # CPUä½¿ç”¨ç‡
            cpu_usage = psutil.cpu_percent(interval=1)
            
            # å†…å­˜ä½¿ç”¨æƒ…å†µ
            memory = psutil.virtual_memory()
            process = psutil.Process()
            memory_peak = process.memory_info().rss / (1024**2)  # MB
            
            # ç£ç›˜I/O
            disk_io = psutil.disk_io_counters()
            disk_read = disk_io.read_bytes if disk_io else 0
            disk_write = disk_io.write_bytes if disk_io else 0
            
            # ç½‘ç»œI/O
            network_io = psutil.net_io_counters()
            net_sent = network_io.bytes_sent if network_io else 0
            net_recv = network_io.bytes_recv if network_io else 0
            
            # è¿›ç¨‹å’Œçº¿ç¨‹æ•°
            process_count = len(psutil.pids())
            thread_count = threading.active_count()
            
            return PerformanceMetrics(
                timestamp=datetime.now().isoformat(),
                cpu_usage=cpu_usage,
                memory_usage=memory.percent,
                memory_peak=memory_peak,
                disk_io_read=disk_read,
                disk_io_write=disk_write,
                network_io_sent=net_sent,
                network_io_recv=net_recv,
                process_count=process_count,
                thread_count=thread_count
            )
        except Exception as e:
            print(f"âŒ æ”¶é›†æ€§èƒ½æŒ‡æ ‡å¤±è´¥: {e}")
            return None
    
    def start_performance_monitoring(self, interval: int = 5, duration: int = 60):
        """å¯åŠ¨æ€§èƒ½ç›‘æ§"""
        if self.monitoring_active:
            print("âš ï¸  æ€§èƒ½ç›‘æ§å·²åœ¨è¿è¡Œ")
            return
        
        self.monitoring_active = True
        self.performance_history.clear()
        
        def monitor_loop():
            start_time = time.time()
            while self.monitoring_active and (time.time() - start_time) < duration:
                metrics = self.collect_performance_metrics()
                if metrics:
                    self.performance_history.append(metrics)
                time.sleep(interval)
            self.monitoring_active = False
        
        self.monitor_thread = threading.Thread(target=monitor_loop)
        self.monitor_thread.daemon = True
        self.monitor_thread.start()
        
        print(f"ğŸš€ æ€§èƒ½ç›‘æ§å·²å¯åŠ¨ï¼Œé—´éš”: {interval}ç§’ï¼ŒæŒç»­æ—¶é—´: {duration}ç§’")
    
    def stop_performance_monitoring(self):
        """åœæ­¢æ€§èƒ½ç›‘æ§"""
        self.monitoring_active = False
        if self.monitor_thread:
            self.monitor_thread.join(timeout=5)
        print("â¹ï¸  æ€§èƒ½ç›‘æ§å·²åœæ­¢")
    
    def analyze_performance(self) -> Dict[str, Any]:
        """åˆ†ææ€§èƒ½æ•°æ®"""
        if not self.performance_history:
            return {"error": "æ²¡æœ‰æ€§èƒ½æ•°æ®"}
        
        # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
        cpu_values = [m.cpu_usage for m in self.performance_history]
        memory_values = [m.memory_usage for m in self.performance_history]
        memory_peak_values = [m.memory_peak for m in self.performance_history]
        
        analysis = {
            "monitoring_period": {
                "start": self.performance_history[0].timestamp,
                "end": self.performance_history[-1].timestamp,
                "duration_seconds": len(self.performance_history) * 5,  # å‡è®¾5ç§’é—´éš”
                "sample_count": len(self.performance_history)
            },
            "cpu_analysis": {
                "average": sum(cpu_values) / len(cpu_values),
                "maximum": max(cpu_values),
                "minimum": min(cpu_values),
                "trend": self._calculate_trend(cpu_values)
            },
            "memory_analysis": {
                "average_usage_percent": sum(memory_values) / len(memory_values),
                "maximum_usage_percent": max(memory_values),
                "average_peak_mb": sum(memory_peak_values) / len(memory_peak_values),
                "maximum_peak_mb": max(memory_peak_values),
                "trend": self._calculate_trend(memory_values)
            },
            "performance_issues": self._identify_performance_issues(),
            "recommendations": self._generate_recommendations()
        }
        
        return analysis
    
    def _calculate_trend(self, values: List[float]) -> str:
        """è®¡ç®—è¶‹åŠ¿"""
        if len(values) < 2:
            return "ç¨³å®š"
        
        # ç®€å•çº¿æ€§è¶‹åŠ¿åˆ†æ
        first_half = values[:len(values)//2]
        second_half = values[len(values)//2:]
        
        first_avg = sum(first_half) / len(first_half)
        second_avg = sum(second_half) / len(second_half)
        
        if second_avg > first_avg * 1.1:
            return "ä¸Šå‡"
        elif second_avg < first_avg * 0.9:
            return "ä¸‹é™"
        else:
            return "ç¨³å®š"
    
    def _identify_performance_issues(self) -> List[Dict[str, Any]]:
        """è¯†åˆ«æ€§èƒ½é—®é¢˜"""
        issues = []
        
        if not self.performance_history:
            return issues
        
        # CPUä½¿ç”¨ç‡é—®é¢˜
        cpu_values = [m.cpu_usage for m in self.performance_history]
        avg_cpu = sum(cpu_values) / len(cpu_values)
        max_cpu = max(cpu_values)
        
        if avg_cpu > 80:
            issues.append({
                "type": "cpu_usage",
                "severity": "high" if avg_cpu > 90 else "medium",
                "description": f"CPUä½¿ç”¨ç‡è¿‡é«˜ï¼Œå¹³å‡: {avg_cpu:.1f}%",
                "recommendation": "ä¼˜åŒ–CPUå¯†é›†å‹æ“ä½œï¼Œè€ƒè™‘å¹¶è¡Œå¤„ç†"
            })
        
        if max_cpu > 95:
            issues.append({
                "type": "cpu_peak",
                "severity": "critical",
                "description": f"CPUä½¿ç”¨ç‡å³°å€¼è¿‡é«˜: {max_cpu:.1f}%",
                "recommendation": "æ£€æŸ¥æ˜¯å¦æœ‰CPUå¯†é›†å‹ä»»åŠ¡éœ€è¦ä¼˜åŒ–"
            })
        
        # å†…å­˜ä½¿ç”¨é—®é¢˜
        memory_values = [m.memory_usage for m in self.performance_history]
        avg_memory = sum(memory_values) / len(memory_values)
        max_memory = max(memory_values)
        
        if avg_memory > 85:
            issues.append({
                "type": "memory_usage",
                "severity": "high" if avg_memory > 95 else "medium",
                "description": f"å†…å­˜ä½¿ç”¨ç‡è¿‡é«˜ï¼Œå¹³å‡: {avg_memory:.1f}%",
                "recommendation": "ä¼˜åŒ–å†…å­˜ä½¿ç”¨ï¼Œæ£€æŸ¥å†…å­˜æ³„æ¼"
            })
        
        # å†…å­˜å³°å€¼é—®é¢˜
        memory_peak_values = [m.memory_peak for m in self.performance_history]
        max_peak = max(memory_peak_values)
        
        if max_peak > 1000:  # 1GB
            issues.append({
                "type": "memory_peak",
                "severity": "medium",
                "description": f"å†…å­˜å³°å€¼è¿‡é«˜: {max_peak:.1f}MB",
                "recommendation": "ä¼˜åŒ–å†…å­˜åˆ†é…ï¼Œè€ƒè™‘ä½¿ç”¨ç”Ÿæˆå™¨æˆ–æµå¼å¤„ç†"
            })
        
        return issues
    
    def _generate_recommendations(self) -> List[str]:
        """ç”Ÿæˆä¼˜åŒ–å»ºè®®"""
        recommendations = []
        
        if not self.performance_history:
            return recommendations
        
        # åŸºäºæ€§èƒ½æ•°æ®ç”Ÿæˆå»ºè®®
        cpu_values = [m.cpu_usage for m in self.performance_history]
        memory_values = [m.memory_usage for m in self.performance_history]
        
        avg_cpu = sum(cpu_values) / len(cpu_values)
        avg_memory = sum(memory_values) / len(memory_values)
        
        if avg_cpu > 70:
            recommendations.extend([
                "è€ƒè™‘ä½¿ç”¨å¤šçº¿ç¨‹æˆ–å¤šè¿›ç¨‹å¹¶è¡Œå¤„ç†",
                "ä¼˜åŒ–ç®—æ³•å¤æ‚åº¦ï¼Œå‡å°‘CPUå¯†é›†å‹æ“ä½œ",
                "ä½¿ç”¨ç¼“å­˜å‡å°‘é‡å¤è®¡ç®—"
            ])
        
        if avg_memory > 80:
            recommendations.extend([
                "ä¼˜åŒ–å†…å­˜ä½¿ç”¨ï¼ŒåŠæ—¶é‡Šæ”¾ä¸éœ€è¦çš„å¯¹è±¡",
                "ä½¿ç”¨ç”Ÿæˆå™¨æ›¿ä»£åˆ—è¡¨ï¼Œå‡å°‘å†…å­˜å ç”¨",
                "è€ƒè™‘åˆ†æ‰¹å¤„ç†å¤§æ–‡ä»¶"
            ])
        
        # é€šç”¨å»ºè®®
        recommendations.extend([
            "å®šæœŸæ¸…ç†ä¸´æ—¶æ–‡ä»¶å’Œç¼“å­˜",
            "ä½¿ç”¨æ€§èƒ½åˆ†æå·¥å…·è¯†åˆ«ç“¶é¢ˆ",
            "è€ƒè™‘ä½¿ç”¨æ›´é«˜æ•ˆçš„æ•°æ®ç»“æ„"
        ])
        
        return list(set(recommendations))  # å»é‡
    
    def profile_function(self, func: Callable, *args, **kwargs) -> Dict[str, Any]:
        """åˆ†æå‡½æ•°æ€§èƒ½"""
        try:
            # å¼€å§‹æ€§èƒ½åˆ†æ
            pr = cProfile.Profile()
            pr.enable()
            
            # æ‰§è¡Œå‡½æ•°
            start_time = time.time()
            result = func(*args, **kwargs)
            end_time = time.time()
            
            # åœæ­¢æ€§èƒ½åˆ†æ
            pr.disable()
            
            # è·å–åˆ†æç»“æœ
            s = io.StringIO()
            ps = pstats.Stats(pr, stream=s).sort_stats('cumulative')
            ps.print_stats()
            profile_output = s.getvalue()
            
            return {
                "function_name": func.__name__,
                "execution_time": end_time - start_time,
                "result": result,
                "profile_output": profile_output,
                "timestamp": datetime.now().isoformat()
            }
            
        except Exception as e:
            return {
                "function_name": func.__name__,
                "error": str(e),
                "timestamp": datetime.now().isoformat()
            }
    
    def optimize_file_processing(self, file_paths: List[Path], 
                                processing_func: Callable) -> OptimizationResult:
        """ä¼˜åŒ–æ–‡ä»¶å¤„ç†æ€§èƒ½"""
        try:
            # è®°å½•ä¼˜åŒ–å‰çš„æŒ‡æ ‡
            before_metrics = self.collect_performance_metrics()
            before_time = time.time()
            
            # åŸå§‹å¤„ç†æ–¹å¼
            results = []
            for file_path in file_paths:
                result = processing_func(file_path)
                results.append(result)
            
            before_duration = time.time() - before_time
            
            # ä¼˜åŒ–åçš„å¤„ç†æ–¹å¼ï¼ˆå¹¶è¡Œå¤„ç†ï¼‰
            after_time = time.time()
            
            # ä½¿ç”¨çº¿ç¨‹æ± å¹¶è¡Œå¤„ç†
            with ThreadPoolExecutor(max_workers=min(len(file_paths), multiprocessing.cpu_count())) as executor:
                future_results = [executor.submit(processing_func, file_path) for file_path in file_paths]
                optimized_results = [future.result() for future in future_results]
            
            after_duration = time.time() - after_time
            
            # è®°å½•ä¼˜åŒ–åçš„æŒ‡æ ‡
            after_metrics = self.collect_performance_metrics()
            
            # è®¡ç®—æ”¹è¿›ç™¾åˆ†æ¯”
            improvement = ((before_duration - after_duration) / before_duration * 100) if before_duration > 0 else 0
            
            optimization_result = OptimizationResult(
                optimization_type="parallel_processing",
                before_metrics={
                    "duration": before_duration,
                    "cpu_usage": before_metrics.cpu_usage if before_metrics else 0,
                    "memory_usage": before_metrics.memory_usage if before_metrics else 0
                },
                after_metrics={
                    "duration": after_duration,
                    "cpu_usage": after_metrics.cpu_usage if after_metrics else 0,
                    "memory_usage": after_metrics.memory_usage if after_metrics else 0
                },
                improvement_percent=improvement,
                description=f"ä½¿ç”¨å¹¶è¡Œå¤„ç†ä¼˜åŒ–æ–‡ä»¶å¤„ç†ï¼Œå¤„ç†äº†{len(file_paths)}ä¸ªæ–‡ä»¶",
                recommendations=[
                    "ä½¿ç”¨ThreadPoolExecutorè¿›è¡Œå¹¶è¡Œå¤„ç†",
                    "æ ¹æ®CPUæ ¸å¿ƒæ•°è°ƒæ•´çº¿ç¨‹æ± å¤§å°",
                    "è€ƒè™‘ä½¿ç”¨ProcessPoolExecutorå¤„ç†CPUå¯†é›†å‹ä»»åŠ¡"
                ]
            )
            
            self.optimization_results.append(optimization_result)
            
            return optimization_result
            
        except Exception as e:
            return OptimizationResult(
                optimization_type="parallel_processing",
                before_metrics={},
                after_metrics={},
                improvement_percent=0,
                description=f"ä¼˜åŒ–å¤±è´¥: {str(e)}",
                recommendations=["æ£€æŸ¥é”™è¯¯æ—¥å¿—ï¼Œä¿®å¤é—®é¢˜åé‡è¯•"]
            )
    
    def optimize_memory_usage(self, data_processing_func: Callable, 
                             data_source: Any) -> OptimizationResult:
        """ä¼˜åŒ–å†…å­˜ä½¿ç”¨"""
        try:
            # è®°å½•ä¼˜åŒ–å‰çš„å†…å­˜ä½¿ç”¨
            before_metrics = self.collect_performance_metrics()
            before_memory = before_metrics.memory_peak if before_metrics else 0
            
            # å¼ºåˆ¶åƒåœ¾å›æ”¶
            gc.collect()
            
            # æ‰§è¡Œä¼˜åŒ–å‰çš„å¤„ç†
            before_time = time.time()
            result = data_processing_func(data_source)
            before_duration = time.time() - before_time
            
            # æ¸…ç†å†…å­˜
            del result
            gc.collect()
            
            # è®°å½•ä¼˜åŒ–åçš„å†…å­˜ä½¿ç”¨
            after_metrics = self.collect_performance_metrics()
            after_memory = after_metrics.memory_peak if after_metrics else 0
            
            # è®¡ç®—å†…å­˜ä½¿ç”¨æ”¹è¿›
            memory_improvement = ((before_memory - after_memory) / before_memory * 100) if before_memory > 0 else 0
            
            optimization_result = OptimizationResult(
                optimization_type="memory_optimization",
                before_metrics={
                    "memory_peak_mb": before_memory,
                    "duration": before_duration
                },
                after_metrics={
                    "memory_peak_mb": after_memory,
                    "duration": before_duration
                },
                improvement_percent=memory_improvement,
                description="é€šè¿‡åƒåœ¾å›æ”¶å’Œå†…å­˜ç®¡ç†ä¼˜åŒ–å†…å­˜ä½¿ç”¨",
                recommendations=[
                    "åŠæ—¶é‡Šæ”¾ä¸éœ€è¦çš„å¯¹è±¡",
                    "ä½¿ç”¨ç”Ÿæˆå™¨æ›¿ä»£åˆ—è¡¨",
                    "å®šæœŸè°ƒç”¨gc.collect()æ¸…ç†å†…å­˜"
                ]
            )
            
            self.optimization_results.append(optimization_result)
            
            return optimization_result
            
        except Exception as e:
            return OptimizationResult(
                optimization_type="memory_optimization",
                before_metrics={},
                after_metrics={},
                improvement_percent=0,
                description=f"å†…å­˜ä¼˜åŒ–å¤±è´¥: {str(e)}",
                recommendations=["æ£€æŸ¥å†…å­˜ä½¿ç”¨æ¨¡å¼ï¼Œè¯†åˆ«å†…å­˜æ³„æ¼"]
            )
    
    def benchmark_tool_performance(self, tool_path: Path, test_data: str) -> Dict[str, Any]:
        """åŸºå‡†æµ‹è¯•å·¥å…·æ€§èƒ½"""
        try:
            # åˆ›å»ºæµ‹è¯•æ•°æ®
            with tempfile.NamedTemporaryFile(mode='w', suffix='.md', delete=False, encoding='utf-8') as f:
                f.write(test_data)
                test_file = f.name
            
            # è¿è¡ŒåŸºå‡†æµ‹è¯•
            start_time = time.time()
            start_memory = psutil.Process().memory_info().rss / (1024**2)
            
            result = subprocess.run([
                sys.executable, str(tool_path), test_file
            ], capture_output=True, text=True, timeout=60)
            
            end_time = time.time()
            end_memory = psutil.Process().memory_info().rss / (1024**2)
            
            # æ¸…ç†æµ‹è¯•æ–‡ä»¶
            try:
                os.unlink(test_file)
            except Exception:
                pass
            
            return {
                "tool_name": tool_path.name,
                "execution_time": end_time - start_time,
                "memory_usage_mb": end_memory - start_memory,
                "return_code": result.returncode,
                "success": result.returncode == 0,
                "output_size": len(result.stdout),
                "error_size": len(result.stderr),
                "timestamp": datetime.now().isoformat()
            }
            
        except Exception as e:
            return {
                "tool_name": tool_path.name,
                "error": str(e),
                "timestamp": datetime.now().isoformat()
            }
    
    def generate_optimization_report(self) -> Dict[str, Any]:
        """ç”Ÿæˆä¼˜åŒ–æŠ¥å‘Š"""
        try:
            performance_analysis = self.analyze_performance()
            
            report = {
                "report_time": datetime.now().isoformat(),
                "performance_analysis": performance_analysis,
                "optimization_results": [asdict(result) for result in self.optimization_results],
                "recommendations": {
                    "immediate": [],
                    "short_term": [],
                    "long_term": []
                },
                "summary": {
                    "total_optimizations": len(self.optimization_results),
                    "average_improvement": sum(r.improvement_percent for r in self.optimization_results) / len(self.optimization_results) if self.optimization_results else 0,
                    "performance_issues_count": len(performance_analysis.get("performance_issues", [])),
                    "recommendations_count": len(performance_analysis.get("recommendations", []))
                }
            }
            
            # åˆ†ç±»å»ºè®®
            if performance_analysis.get("performance_issues"):
                for issue in performance_analysis["performance_issues"]:
                    if issue["severity"] == "critical":
                        report["recommendations"]["immediate"].append(issue["recommendation"])
                    elif issue["severity"] == "high":
                        report["recommendations"]["short_term"].append(issue["recommendation"])
                    else:
                        report["recommendations"]["long_term"].append(issue["recommendation"])
            
            return report
            
        except Exception as e:
            return {"error": f"ç”Ÿæˆä¼˜åŒ–æŠ¥å‘Šå¤±è´¥: {str(e)}"}
    
    def save_report(self, report: Dict[str, Any], filename: str = None) -> bool:
        """ä¿å­˜æŠ¥å‘Š"""
        try:
            if filename is None:
                timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                filename = f"performance_report_{timestamp}.json"
            
            filepath = self.reports_dir / filename
            
            with open(filepath, 'w', encoding='utf-8') as f:
                json.dump(report, f, indent=2, ensure_ascii=False)
            
            print(f"âœ… æ€§èƒ½æŠ¥å‘Šå·²ä¿å­˜: {filepath}")
            return True
            
        except Exception as e:
            print(f"âŒ ä¿å­˜æ€§èƒ½æŠ¥å‘Šå¤±è´¥: {e}")
            return False

def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='æ€§èƒ½ä¼˜åŒ–å·¥å…·')
    parser.add_argument('--root', default='.', help='æ ¹ç›®å½•è·¯å¾„')
    parser.add_argument('--monitor', action='store_true', help='å¯åŠ¨æ€§èƒ½ç›‘æ§')
    parser.add_argument('--analyze', action='store_true', help='åˆ†ææ€§èƒ½æ•°æ®')
    parser.add_argument('--optimize', help='è¿è¡Œä¼˜åŒ–æµ‹è¯•')
    parser.add_argument('--benchmark', help='åŸºå‡†æµ‹è¯•å·¥å…·')
    parser.add_argument('--report', action='store_true', help='ç”Ÿæˆä¼˜åŒ–æŠ¥å‘Š')
    parser.add_argument('--interval', type=int, default=5, help='ç›‘æ§é—´éš”ï¼ˆç§’ï¼‰')
    parser.add_argument('--duration', type=int, default=60, help='ç›‘æ§æŒç»­æ—¶é—´ï¼ˆç§’ï¼‰')
    
    args = parser.parse_args()
    
    optimizer = PerformanceOptimizer(args.root)
    
    print("=" * 50)
    print("ğŸš€ æ€§èƒ½ä¼˜åŒ–å·¥å…·")
    print("=" * 50)
    
    if args.monitor:
        optimizer.start_performance_monitoring(args.interval, args.duration)
        try:
            time.sleep(args.duration)
        except KeyboardInterrupt:
            pass
        finally:
            optimizer.stop_performance_monitoring()
    elif args.analyze:
        analysis = optimizer.analyze_performance()
        print(json.dumps(analysis, indent=2, ensure_ascii=False))
    elif args.optimize:
        print(f"è¿è¡Œä¼˜åŒ–æµ‹è¯•: {args.optimize}")
    elif args.benchmark:
        tool_path = Path(args.benchmark)
        if tool_path.exists():
            test_data = "# æµ‹è¯•æ–‡æ¡£\n\n## ç« èŠ‚1\n\næµ‹è¯•å†…å®¹ã€‚\n"
            result = optimizer.benchmark_tool_performance(tool_path, test_data)
            print(json.dumps(result, indent=2, ensure_ascii=False))
        else:
            print(f"âŒ å·¥å…·ä¸å­˜åœ¨: {tool_path}")
    elif args.report:
        report = optimizer.generate_optimization_report()
        optimizer.save_report(report)
    else:
        print("è¯·æŒ‡å®šæ“ä½œ")
        print("ä½¿ç”¨ --help æŸ¥çœ‹è¯¦ç»†å¸®åŠ©")

if __name__ == "__main__":
    main()
